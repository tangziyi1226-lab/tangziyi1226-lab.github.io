<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>11.16 论文记录</title>
    <link href="/2025/11/16/11-16-%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/"/>
    <url>/2025/11/16/11-16-%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h2 id="核心信息解读"><a href="#核心信息解读" class="headerlink" title="核心信息解读"></a>核心信息解读</h2><h3 id="一、模型核心创新点"><a href="#一、模型核心创新点" class="headerlink" title="一、模型核心创新点"></a>一、模型核心创新点</h3><h4 id="1-整流流（Rectified-Flow-RF）的噪声采样优化"><a href="#1-整流流（Rectified-Flow-RF）的噪声采样优化" class="headerlink" title="1. 整流流（Rectified Flow, RF）的噪声采样优化"></a>1. 整流流（Rectified Flow, RF）的噪声采样优化</h4><p>针对传统RF模型对时间步$t \in [0,1]$均匀采样导致中间时间步（优化难度最高）训练不足的问题，提出三种<strong>感知相关尺度偏向的时间步采样策略</strong>，核心是通过非均匀分布$\pi(t)$增加中间时间步权重，提升模型对复杂生成过程的学习能力： </p><ul><li><strong>Logit-Normal采样</strong>：通过位置参数$m$（控制偏向数据&#x2F;噪声）和尺度参数$s$（控制分布宽度），将采样集中于中间步，公式为$\pi_{ln}(t;m,s)&#x3D;\frac{1}{s\sqrt{2\pi}}\frac{1}{t(1-t)}\exp\left(-\frac{(logit(t)-m)^2}{2s^2}\right)$（$logit(t)&#x3D;log\frac{t}{1-t}$），实验中$rf&#x2F;lognorm(0.00,1.00)$（$m&#x3D;0$、$s&#x3D;1$）表现最优。 </li><li><strong>Mode重尾采样</strong>：定义单调函数$f_{mode}(u;s)&#x3D;1-u-s\cdot\left(cos^2\left(\frac{\pi}{2}u\right)-1+u\right)$，确保端点步（$t&#x3D;0$或$t&#x3D;1$）有非零密度，避免极端时间步训练缺失，$s&#x3D;0$时退化为均匀采样。 </li><li><strong>CosMap采样</strong>：映射$u \mapsto t$使log-SNR匹配余弦调度，推导密度$\pi_{CosMap}(t)&#x3D;\frac{2}{\pi-2\pi t+2\pi t^2}$，继承余弦调度的稳定性。</li></ul><h4 id="2-多模态Transformer架构（MM-DiT）"><a href="#2-多模态Transformer架构（MM-DiT）" class="headerlink" title="2. 多模态Transformer架构（MM-DiT）"></a>2. 多模态Transformer架构（MM-DiT）</h4><ul><li>突破传统“固定文本表示→交叉注意力注入”的局限，实现<strong>图文模态双向信息流动与权重分离</strong>： </li><li><strong>双模态独立权重</strong>：为图像、文本token分别设计独立Transformer权重，仅在注意力操作时合并序列，既保留模态特异性（如图像空间特征、文本语义特征），又支持跨模态交互。 </li><li><strong>多文本编码器融合</strong>：使用CLIP-L&#x2F;14、CLIP-G&#x2F;14、T5-XXL三个预训练模型编码文本，输出全局向量$c_{vec}$（2048维）和序列表示$c_{ctxt}$（154×4096维），适配不同复杂度的文本理解需求（如T5对长文本&#x2F;细节描述至关重要）。 </li><li><strong>时间步与文本调制</strong>：将时间步$t$的正弦编码和文本向量$c_{vec}$注入注意力与MLP层，增强模型对生成时序和文本条件的适应性。 ### 3. 高分辨率训练与优化技巧 </li><li><strong>改进自编码器</strong>：提升 latent 通道数$d$从4→16，重建FID从2.41降至1.06，为高分辨率生成提供更精细的 latent 基础。 </li><li><strong>分辨率自适应时间步偏移</strong>：推导公式$t_m&#x3D;\frac{\sqrt{m&#x2F;n}t_n}{1+(\sqrt{m&#x2F;n}-1)t_n}$，将低分辨率（$n$像素）时间步$t_n$映射到高分辨率（$m$像素）$t_m$，保证噪声不确定性一致，1024×1024训练中设偏移系数$\alpha&#x3D;3.0$。</li><li><strong>QK归一化</strong>：注意力计算前对Q&#x2F;K向量做RMSNorm，解决高分辨率混合精度训练中注意力熵失控问题，支持bf16精度训练（性能仅下降~2×）。 </li><li><h3 id="二、模型流程（以文本到高分辨率图像生成为例）"><a href="#二、模型流程（以文本到高分辨率图像生成为例）" class="headerlink" title="二、模型流程（以文本到高分辨率图像生成为例）"></a>二、模型流程（以文本到高分辨率图像生成为例）</h3></li><li><h4 id="1-输入处理"><a href="#1-输入处理" class="headerlink" title="1. 输入处理"></a>1. 输入处理</h4></li><li><strong>文本输入</strong>：通过CLIP-L&#x2F;14、CLIP-G&#x2F;14、T5-XXL编码，得到： - 全局向量$c_{vec}$：拼接CLIP-L&#x2F;14（768维）和CLIP-G&#x2F;14（1280维）的池化输出，共2048维。 - 序列表示$c_{ctxt}$：CLIP序列（77×2048维）零填充至4096维后，与T5序列（77×4096维）拼接，共154×4096维。 </li><li><strong>图像生成初始化</strong>：从标准正态分布$p_1$采样噪声$x_1$（ latent 空间，下采样因子8，如1024×1024图像对应128×128×16 latent ）。</li><li><h4 id="2-训练阶段（基于条件流匹配CFM）"><a href="#2-训练阶段（基于条件流匹配CFM）" class="headerlink" title="2. 训练阶段（基于条件流匹配CFM）"></a>2. 训练阶段（基于条件流匹配CFM）</h4></li></ul><ol><li><strong>时间步采样</strong>：从Logit-Normal&#x2F;Mode&#x2F;CosMap分布采样$t \in [0,1]$，生成混合样本$z_t&#x3D;(1-t)x_0 + t\epsilon$（$x_0$为真实图像的 latent ，$\epsilon \sim \mathcal{N}(0,I)$）。 </li><li><strong>MM-DiT前向</strong>： - 图像 latent 展平为2×2 patches，添加位置编码后与$c_{ctxt}$拼接，映射到统一维度。 - 经过$d$个MM-DiT块（含模态独立注意力、MLP、时间步&#x2F;文本调制），输出 velocity $v_\Theta(z_t,t)$。 </li><li><strong>损失计算</strong>：使用条件流匹配损失$\mathcal{L}<em>{CFM}&#x3D;\mathbb{E}</em>{t,p_t(z|\epsilon),p(\epsilon)}\left|v_\Theta(z,t)-u_t(z|\epsilon)\right|_2^2$，其中$u_t(z|\epsilon)$为理论 velocity（见关键公式3）。</li></ol><h4 id="3-推理阶段（ODE求解）"><a href="#3-推理阶段（ODE求解）" class="headerlink" title="3. 推理阶段（ODE求解）"></a>3. 推理阶段（ODE求解）</h4><ol><li><strong>时间步偏移</strong>：根据目标分辨率（如1024×1024），使用公式$t_m&#x3D;\frac{\sqrt{m&#x2F;n}t_n}{1+(\sqrt{m&#x2F;n}-1)t_n}$调整采样时间步。 </li><li><strong>ODE离散求解</strong>：从噪声$x_1$出发，通过Euler方法迭代求解$dy_t&#x3D;v_\Theta(y_t,t)dt$，逐步生成$y_{t-1},y_{t-2},…,y_0$（$y_0$为目标图像的 latent ）。 </li><li><strong>图像解码</strong>：将$y_0$输入自编码器解码器，得到最终高分辨率RGB图像（如1024×1024）。</li></ol><h3 id="三、输入输出定义"><a href="#三、输入输出定义" class="headerlink" title="三、输入输出定义"></a>三、输入输出定义</h3><table><thead><tr><th>模块</th><th>输入内容</th><th>输出内容</th><th>说明</th></tr></thead><tbody><tr><td>文本编码器</td><td>自然语言 prompt（如 “A cat wearing a hat”）</td><td>cvec​（2048 维）、cctxt​（154×4096 维）</td><td>预训练模型冻结，提供文本语义表示</td></tr><tr><td>自编码器（训练时）</td><td>RGB 图像（如 256×256&#x2F;1024×1024）</td><td>latent x0​（如 32×32×16&#x2F;128×128×16）</td><td>下采样因子 8，d&#x3D;16通道</td></tr><tr><td>MM-DiT（训练）</td><td>zt​（latent 混合样本）、t（时间步）、cvec​、cctxt​</td><td>velocity vΘ​(zt​,t)（与zt​同维度）</td><td>学习从噪声到数据的 velocity 场</td></tr><tr><td>MM-DiT（推理）</td><td>噪声x1​、t（调整后时间步）、cvec​、cctxt​</td><td>velocity vΘ​(yt​,t)</td><td>用于 ODE 迭代更新 latent</td></tr><tr><td>自编码器（推理时）</td><td>生成的 latent y0​</td><td>高分辨率 RGB 图像（如 1024×1024）</td><td>从 latent 空间恢复像素空间</td></tr></tbody></table><p>编辑</p><p>深度思考</p><h3 id="四、关键公式解读"><a href="#四、关键公式解读" class="headerlink" title="四、关键公式解读"></a>四、关键公式解读</h3><h4 id="1-整流流前向过程（数据-噪声直线路径）"><a href="#1-整流流前向过程（数据-噪声直线路径）" class="headerlink" title="1. 整流流前向过程（数据-噪声直线路径）"></a>1. 整流流前向过程（数据-噪声直线路径）</h4><p>$$z_t&#x3D;(1-t)x_0 + t\epsilon \quad (1)$$ - <strong>含义</strong>：定义时间步$t$时的混合样本$z_t$，$t&#x3D;0$时为真实数据$x_0$，$t&#x3D;1$时为噪声$\epsilon$，路径为直线（区别于扩散模型的弯曲路径）。 - <strong>优势</strong>：直线路径可减少ODE求解步数，提升采样效率（少步采样时性能下降更少，见图3）。 </p><h4 id="2-条件向量场（理论velocity）"><a href="#2-条件向量场（理论velocity）" class="headerlink" title="2. 条件向量场（理论velocity）"></a>2. 条件向量场（理论velocity）</h4><p>$$u_t(z|\epsilon)&#x3D;\frac{a_t’}{a_t}z - \frac{b_t}{2}\lambda_t’\epsilon \quad (2)$$ - <strong>推导基础</strong>：$a_t&#x3D;1-t$、$b_t&#x3D;t$（整流流设定），信号噪声比$\lambda_t&#x3D;log\frac{a_t^2}{b_t^2}$，$\lambda_t’&#x3D;2\left(\frac{a_t’}{a_t}-\frac{b_t’}{b_t}\right)$。 - <strong>简化结果</strong>：代入$a_t&#x3D;1-t$、$b_t&#x3D;t$后，$u_t(z|\epsilon)&#x3D;-\frac{1}{1-t}z + \frac{t}{1-t}\epsilon$，表示$z_t$沿直线路径的理论速度。 </p><h4 id="3-条件流匹配损失（训练目标）"><a href="#3-条件流匹配损失（训练目标）" class="headerlink" title="3. 条件流匹配损失（训练目标）"></a>3. 条件流匹配损失（训练目标）</h4><p>$$\mathcal{L}<em>{CFM}&#x3D;\mathbb{E}</em>{t,p_t(z|\epsilon),p(\epsilon)}\left|v_\Theta(z,t)-u_t(z|\epsilon)\right|<em>2^2 \quad (3)$$ - <strong>含义</strong>：最小化模型预测的 velocity $v</em>\Theta(z,t)$与理论 velocity $u_t(z|\epsilon)$的L2距离，避免直接计算边际向量场$u_t(z)$（intractable）。 - <strong>优势</strong>：等价于流匹配损失$\mathcal{L}_{FM}$（见附录B.1），但计算可行，且能保证学习到的向量场生成正确的概率路径$p_t$。 ### 4. 分辨率自适应时间步偏移 $$t_m&#x3D;\frac{\sqrt{m&#x2F;n}t_n}{1+\left(\sqrt{m&#x2F;n}-1\right)t_n} \quad (4)$$ - <strong>含义</strong>：将低分辨率$n$（如256×256）的时间步$t_n$映射到高分辨率$m$（如1024×1024）的$t_m$，保证噪声不确定性$\sigma(t,n)&#x3D;\frac{t}{1-t}\sqrt{\frac{1}{n}}$一致。 - <strong>实例</strong>：$m&#x3D;4n$（256→512）时，$t_m&#x3D;\frac{2t_n}{1+t_n}$，确保高分辨率图像需更多噪声才能达到相同不确定性。 </p><h3 id="五、实验结果"><a href="#五、实验结果" class="headerlink" title="五、实验结果"></a>五、实验结果</h3><h4 id="1-整流流采样策略对比（ImageNet-CC12M数据集）"><a href="#1-整流流采样策略对比（ImageNet-CC12M数据集）" class="headerlink" title="1. 整流流采样策略对比（ImageNet+CC12M数据集）"></a>1. 整流流采样策略对比（ImageNet+CC12M数据集）</h4><table><thead><tr><th>模型变体</th><th>平均排名（越低越好）</th><th>5 步采样排名</th><th>50 步采样排名</th><th>ImageNet CLIP（越高越好）</th><th>CC12M FID（越低越好）</th></tr></thead><tbody><tr><td>rf&#x2F;lognorm(0.00,1.00)</td><td>1.54</td><td>1.25</td><td>1.50</td><td>0.250</td><td>89.91</td></tr><tr><td>eps&#x2F;linear（LDM 基线）</td><td>2.88</td><td>4.25</td><td>2.75</td><td>0.245</td><td>90.34</td></tr><tr><td>rf（均匀采样）</td><td>5.67</td><td>6.50</td><td>5.75</td><td>0.247</td><td>94.90</td></tr><tr><td>edm (-1.20,1.20)（EDM 基线）</td><td>15.58</td><td>20.25</td><td>15.00</td><td>0.236</td><td>116.60</td></tr></tbody></table><ul><li><strong>结论</strong>：rf&#x2F;lognorm(0.00,1.00)在少步&#x2F;多步采样、多数据集上均最优，显著超越传统RF和扩散模型基线。 </li><li><h4 id="2-模型架构对比（CC12M数据集，256×256）"><a href="#2-模型架构对比（CC12M数据集，256×256）" class="headerlink" title="2. 模型架构对比（CC12M数据集，256×256）"></a>2. 模型架构对比（CC12M数据集，256×256）</h4></li></ul><table><thead><tr><th>架构</th><th>验证损失（越低越好）</th><th>CLIP 分数（越高越好）</th><th>FID（越低越好）</th></tr></thead><tbody><tr><td>MM-DiT（depth&#x3D;38）</td><td>0.38</td><td>0.42</td><td>45.78</td></tr><tr><td>CrossDiT</td><td>0.40</td><td>0.40</td><td>50.12</td></tr><tr><td>UViT</td><td>0.41</td><td>0.39</td><td>52.34</td></tr><tr><td>DiT（vanilla）</td><td>0.43</td><td>0.37</td><td>55.60</td></tr></tbody></table><ul><li><strong>结论</strong>：MM-DiT因模态权重分离和双向信息流动，收敛速度和最终性能均优于现有Transformer架构。 </li><li><h4 id="3-大规模模型性能（1024×1024，GenEval基准）"><a href="#3-大规模模型性能（1024×1024，GenEval基准）" class="headerlink" title="3. 大规模模型性能（1024×1024，GenEval基准）"></a>3. 大规模模型性能（1024×1024，GenEval基准）</h4></li></ul><table><thead><tr><th>模型</th><th>总体得分（越高越好）</th><th>双物体识别（越高越好）</th><th>颜色属性（越高越好）</th><th>排版生成（越高越好）</th></tr></thead><tbody><tr><td>Ours（depth&#x3D;38，8B）</td><td>0.74</td><td>0.94</td><td>0.60</td><td>0.47</td></tr><tr><td>DALL-E 3</td><td>0.67</td><td>0.87</td><td>0.45</td><td>0.43</td></tr><tr><td>SDXL</td><td>0.55</td><td>0.74</td><td>0.23</td><td>0.15</td></tr><tr><td>PixArt-α</td><td>0.48</td><td>0.50</td><td>0.07</td><td>0.08</td></tr></tbody></table><ul><li><strong>结论</strong>：8B参数模型在所有指标上超越DALL-E 3等闭源模型及SDXL等开源模型，尤其在细节理解（如双物体、颜色属性）上优势显著。 </li><li><h4 id="4-人类偏好评估（Parti-prompts基准）"><a href="#4-人类偏好评估（Parti-prompts基准）" class="headerlink" title="4. 人类偏好评估（Parti-prompts基准）"></a>4. 人类偏好评估（Parti-prompts基准）</h4></li></ul><table><thead><tr><th>模型</th><th>视觉美学（胜率 %，越高越好）</th><th>Prompt 遵循（胜率 %，越高越好）</th><th>排版生成（胜率 %，越高越好）</th></tr></thead><tbody><tr><td>Ours（8B）</td><td>68</td><td>72</td><td>65</td></tr><tr><td>DALL-E 3</td><td>52</td><td>58</td><td>51</td></tr><tr><td>SDXL Turbo</td><td>45</td><td>48</td><td>38</td></tr></tbody></table><ul><li><strong>结论</strong>：人类 evaluators 更偏好8B模型生成的图像，尤其在 prompt 精准遵循和排版细节上。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>paper</tag>
      
      <tag>transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2025/11/15/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2025/11/15/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2025/11/15/hello-world/"/>
    <url>/2025/11/15/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
